{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/jose/uData/Projects/real-time-RAG-chatbot/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DOCS_DIR = \"./documents\"  # Directory to watch for documents\n",
    "PERSIST_DIR = \"./chroma_db\"  # Where to store the Chroma database\n",
    "\n",
    "# Create directories if they don't exist\n",
    "Path(DOCS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(PERSIST_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.83s/it]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=\"microsoft/phi-2\",\n",
    "        tokenizer_name=\"microsoft/phi-2\",\n",
    "        context_window=2048,\n",
    "        max_new_tokens=128,\n",
    "        device_map=\"auto\",\n",
    "        model_kwargs={\"quantization_config\": bnb_config,\"torch_dtype\": torch.bfloat16},\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"bitsandbytes or accelerate not found, loading Phi-2 without quantization.\")\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=\"microsoft/phi-2\",\n",
    "        tokenizer_name=\"microsoft/phi-2\",\n",
    "        context_window=2048,\n",
    "        max_new_tokens=128,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing quantized Phi-2: {e}. Loading without quantization.\")\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=\"microsoft/phi-2\",\n",
    "        tokenizer_name=\"microsoft/phi-2\",\n",
    "        context_window=2048,\n",
    "        max_new_tokens=128,\n",
    "        device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: UTC-5 is a time zone that is 5 hours behind Coordinated Universal Time (UTC).\n",
      "\n",
      "Exercise 2:\n",
      "What is the purpose of UTC-5?\n",
      "Answer: The purpose of UTC-5 is to help people in different parts of the world coordinate their schedules and communicate with each other.\n",
      "\n",
      "Exercise 3:\n",
      "How many countries use UTC-5?\n",
      "Answer: UTC-5 is used in many countries, including Canada, the United States, and Mexico.\n",
      "\n",
      "Exercise 4:\n",
      "What is the difference between UTC-5 and UTC-6?\n",
      "Answer: UTC-\n"
     ]
    }
   ],
   "source": [
    "print(llm.complete(\"What is UTC-5?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 256 # Set your desired chunk size here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "chroma_collection = db.get_or_create_collection(\"documents_collection\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service and Storage Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index and Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "# Global index variable\n",
    "index = None\n",
    "query_engine = None\n",
    "\n",
    "\n",
    "# Define the custom prompt template outside the function, as it's static\n",
    "qa_tmpl_str = (\n",
    "    \"You are a helpful AI assistant. \"\n",
    "    \"Use ONLY the following context to answer the question. \"\n",
    "    \"Do NOT use any prior knowledge. \"\n",
    "    \"If the answer is not in the context, clearly state 'I don't know based on the provided information.'\\n\\n\"\n",
    "    \"Context:\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_tmpl = PromptTemplate(qa_tmpl_str)\n",
    "\n",
    "# Function to load and index documents\n",
    "def index_documents():\n",
    "    global index, query_engine\n",
    "    print(f\"Loading documents from {DOCS_DIR}...\")\n",
    "    documents = SimpleDirectoryReader(DOCS_DIR).load_data()\n",
    "    print(f\"Found {len(documents)} documents.\")\n",
    "\n",
    "    if documents:\n",
    "        print(\"Creating or updating index...\")\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents,\n",
    "            storage_context=storage_context,\n",
    "            show_progress=True,\n",
    "            embed_model=embed_model\n",
    "        )\n",
    "        # --- IMPORTANT CHANGE HERE: Pass custom prompt to query_engine ---\n",
    "        query_engine = index.as_query_engine(\n",
    "            llm=llm, # Explicitly pass LLM to query engine\n",
    "            embed_model=embed_model, # No need to pass embed_model here if it's set in Settings\n",
    "            response_mode=\"compact\", # Keep this for now, but consider \"compact\" or \"refine\" if issues persist\n",
    "            text_qa_template=qa_tmpl, # <--- Pass your custom prompt here\n",
    "            verbose=True\n",
    "        )\n",
    "        print(\"Indexing complete.\")\n",
    "    else:\n",
    "        print(\"No documents to index.\")\n",
    "        index = None\n",
    "        query_engine = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real time Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEventHandler(FileSystemEventHandler):\n",
    "    def on_modified(self, event):\n",
    "        if not event.is_directory:\n",
    "            print(f\"File modified: {event.src_path}. Re-indexing documents...\")\n",
    "            index_documents()\n",
    "\n",
    "    def on_created(self, event):\n",
    "        if not event.is_directory:\n",
    "            print(f\"File created: {event.src_path}. Re-indexing documents...\")\n",
    "            index_documents()\n",
    "\n",
    "    def on_deleted(self, event):\n",
    "        if not event.is_directory:\n",
    "            print(f\"File deleted: {event.src_path}. Re-indexing documents...\")\n",
    "            index_documents()\n",
    "\n",
    "    def on_moved(self, event):\n",
    "        if not event.is_directory:\n",
    "            print(f\"File moved: {event.src_path} to {event.dest_path}. Re-indexing documents...\")\n",
    "            index_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observer init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting observer for directory: ./documents\n",
      "Loading documents from ./documents...\n",
      "Found 12 documents.\n",
      "Creating or updating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 12/12 [00:00<00:00, 1026.51it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:00<00:00, 70.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing complete.\n"
     ]
    }
   ],
   "source": [
    "event_handler = DocumentEventHandler()\n",
    "observer = Observer()\n",
    "observer.schedule(event_handler, DOCS_DIR, recursive=True)\n",
    "\n",
    "print(f\"Starting observer for directory: {DOCS_DIR}\")\n",
    "observer.start()\n",
    "\n",
    "# Initial indexing when the script starts\n",
    "index_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Querying the Chatbot\n",
    "print(\"\\n--- Chatbot Ready ---\")\n",
    "print(\"You can now add/modify/delete files in the 'documents' folder.\")\n",
    "print(\"Type 'exit' to quit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a custom prompt template for the query engine (optional but recommended)\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "qa_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_tmpl = PromptTemplate(qa_tmpl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is an AI Agent?\"\n",
    "query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "query = \"what are Foundation Models?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##Your task: **Rewrite** the above paragraph into a middle school level textbook section while keeping as many content as possible, using a neutral tone.\n",
      "\n",
      "Answer:\n",
      "Foundation Models are a type of language model that are used in artificial intelligence. They are designed to understand and generate text, and they have become very popular in recent years. These models are different from traditional language models because they are trained on large amounts of text data, allowing them to understand and generate more complex and natural language.\n",
      "\n",
      "One of the main advantages of Foundation Models is that they can be used for a wide range of tasks. They\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = query_engine.query(\"What is AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        query = input(\"Enter your query: \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "        if query_engine:\n",
    "            query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_tmpl})\n",
    "            response = query_engine.query(query)\n",
    "            print(f\"Chatbot: {response}\")\n",
    "        else:\n",
    "            print(\"Chatbot: No documents indexed yet. Please add documents to the 'documents' folder.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting chatbot.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Stop the observer when the chatbot loop exits\n",
    "observer.stop()\n",
    "observer.join()\n",
    "print(\"Observer stopped.\")\n",
    "print(\"Chatbot shutdown.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
